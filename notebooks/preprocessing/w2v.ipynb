{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db77398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf4a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate ppl high school used bully hot omg love m...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kat andre asshole omg mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new access trading cause need high level opini...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fuck david duke racist think america belong du...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>may say lot hate apologetic army hope choke ev...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breanichole rapeisntokay made gay joke rape jo...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hindustan time report gay rape joke taken obje...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>willy real man know able cook one attractive q...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>notallmen evolving notallmen evolved blameonen...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anirban stupid mamata regime hundred bjp worke...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet   cyberbullying_type\n",
       "0  hate ppl high school used bully hot omg love m...                  age\n",
       "1                          kat andre asshole omg mkr    not_cyberbullying\n",
       "2  new access trading cause need high level opini...                  age\n",
       "3  fuck david duke racist think america belong du...            ethnicity\n",
       "4  may say lot hate apologetic army hope choke ev...  other_cyberbullying\n",
       "5  breanichole rapeisntokay made gay joke rape jo...               gender\n",
       "6  hindustan time report gay rape joke taken obje...               gender\n",
       "7  willy real man know able cook one attractive q...               gender\n",
       "8  notallmen evolving notallmen evolved blameonen...               gender\n",
       "9  anirban stupid mamata regime hundred bjp worke...             religion"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/train_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ddfcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28614, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c366f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_tweet = df.clean_tweet.astype(str)\n",
    "df.clean_tweet.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5367824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values with empty string\n",
    "# to fix decoder error\n",
    "df.clean_tweet = df.clean_tweet.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5368ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate',\n",
       " 'ppl',\n",
       " 'high',\n",
       " 'school',\n",
       " 'used',\n",
       " 'bully',\n",
       " 'hot',\n",
       " 'omg',\n",
       " 'love',\n",
       " 'makeup',\n",
       " 'get',\n",
       " 'ring',\n",
       " 'wan',\n",
       " 'sometime',\n",
       " 'fuck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_preprocess(df.clean_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2ea469c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [hate, ppl, high, school, used, bully, hot, om...\n",
       "1                      [kat, andre, asshole, omg, mkr]\n",
       "2    [new, access, trading, cause, need, high, leve...\n",
       "3    [fuck, david, duke, racist, think, america, be...\n",
       "4    [may, say, lot, hate, apologetic, army, hope, ...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets = df.clean_tweet.apply(simple_preprocess)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cf15bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawning a Word2Vec model\n",
    "model = Word2Vec(window=5, min_count=2)\n",
    "\n",
    "# building vocabulary from entire corpus\n",
    "model.build_vocab(tweets, progress_per=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "831e08a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 5, 28614, 363508)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vector_size, model.workers, model.epochs, model.corpus_count, model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "107090f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1528271, 1817540)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the word2vec\n",
    "model.train(tweets, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d3d2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "# model.save(\"../../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c7f1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a trained model\n",
    "model = Word2Vec.load('../../models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c309226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('middle', 0.965322732925415),\n",
       " ('elementary', 0.9573146104812622),\n",
       " ('high', 0.9516770839691162),\n",
       " ('graduation', 0.9479526877403259),\n",
       " ('relentlessly', 0.9460520148277283),\n",
       " ('confrontation', 0.9452431201934814),\n",
       " ('grade', 0.9413620829582214),\n",
       " ('teased', 0.9374991059303284),\n",
       " ('teacher', 0.9353218674659729),\n",
       " ('tormentor', 0.9342045187950134)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 similar words to bully\n",
    "model.wv.most_similar('bully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3da6cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trayvon', 0.9649240970611572),\n",
       " ('shut', 0.9571725130081177),\n",
       " ('goshawty', 0.9547345042228699),\n",
       " ('sayin', 0.9497713446617126),\n",
       " ('ignorant', 0.9482175707817078),\n",
       " ('stupid', 0.9463467001914978),\n",
       " ('spic', 0.9458626508712769),\n",
       " ('beaner', 0.9441934823989868),\n",
       " ('nigga', 0.9439464807510376),\n",
       " ('subban', 0.9413833022117615)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 similar words to dumb\n",
    "model.wv.most_similar('dumb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0f5d01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97808623"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word-word cosine similarity\n",
    "model.wv.similarity(w1='india', w2='hindu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2050f1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93749917"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word-word cosine similarity\n",
    "model.wv.similarity(w1='bully', w2='teased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76cc96d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14964, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>-0.192781</td>\n",
       "      <td>0.428249</td>\n",
       "      <td>0.163581</td>\n",
       "      <td>0.179935</td>\n",
       "      <td>0.225132</td>\n",
       "      <td>-0.952760</td>\n",
       "      <td>0.405003</td>\n",
       "      <td>1.078805</td>\n",
       "      <td>-0.556530</td>\n",
       "      <td>-0.293762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495027</td>\n",
       "      <td>0.256294</td>\n",
       "      <td>0.081433</td>\n",
       "      <td>0.399876</td>\n",
       "      <td>0.870979</td>\n",
       "      <td>0.357137</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>-0.456908</td>\n",
       "      <td>0.254847</td>\n",
       "      <td>-0.234902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheeran</th>\n",
       "      <td>-0.007434</td>\n",
       "      <td>0.017735</td>\n",
       "      <td>0.014807</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>-0.006980</td>\n",
       "      <td>-0.051160</td>\n",
       "      <td>0.011977</td>\n",
       "      <td>0.061541</td>\n",
       "      <td>-0.026173</td>\n",
       "      <td>-0.011591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026947</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>0.038903</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>-0.015366</td>\n",
       "      <td>-0.003382</td>\n",
       "      <td>-0.010420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discriminated</th>\n",
       "      <td>0.018713</td>\n",
       "      <td>-0.001984</td>\n",
       "      <td>-0.018869</td>\n",
       "      <td>-0.036782</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>-0.091945</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.086454</td>\n",
       "      <td>-0.028530</td>\n",
       "      <td>-0.025232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051572</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.012612</td>\n",
       "      <td>0.063108</td>\n",
       "      <td>0.020124</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>-0.036124</td>\n",
       "      <td>0.023897</td>\n",
       "      <td>-0.022599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typeonegative</th>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.011045</td>\n",
       "      <td>-0.014000</td>\n",
       "      <td>-0.012526</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>-0.005549</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>-0.008973</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022193</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>-0.007446</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>-0.017803</td>\n",
       "      <td>-0.009518</td>\n",
       "      <td>0.023444</td>\n",
       "      <td>-0.012139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objection</th>\n",
       "      <td>-0.001627</td>\n",
       "      <td>-0.002641</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>-0.012354</td>\n",
       "      <td>-0.015650</td>\n",
       "      <td>-0.016477</td>\n",
       "      <td>-0.000673</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.014575</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>-0.007170</td>\n",
       "      <td>-0.018893</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>-0.008676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professora</th>\n",
       "      <td>-0.022106</td>\n",
       "      <td>0.023572</td>\n",
       "      <td>0.024930</td>\n",
       "      <td>0.030998</td>\n",
       "      <td>-0.004385</td>\n",
       "      <td>-0.076845</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.100673</td>\n",
       "      <td>-0.032465</td>\n",
       "      <td>-0.018333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>0.015398</td>\n",
       "      <td>-0.006973</td>\n",
       "      <td>0.032991</td>\n",
       "      <td>0.060261</td>\n",
       "      <td>0.034890</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>-0.051127</td>\n",
       "      <td>0.015242</td>\n",
       "      <td>-0.035894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slasher</th>\n",
       "      <td>-0.006892</td>\n",
       "      <td>0.013456</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>-0.010877</td>\n",
       "      <td>-0.035924</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.038645</td>\n",
       "      <td>-0.014912</td>\n",
       "      <td>-0.003330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>-0.004797</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.030776</td>\n",
       "      <td>0.014187</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>-0.015796</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>-0.021430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>higher</th>\n",
       "      <td>-0.045284</td>\n",
       "      <td>0.134403</td>\n",
       "      <td>0.067103</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.461645</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>0.509531</td>\n",
       "      <td>-0.162674</td>\n",
       "      <td>-0.164502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196889</td>\n",
       "      <td>0.143719</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>0.136243</td>\n",
       "      <td>0.330119</td>\n",
       "      <td>0.117404</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>-0.227228</td>\n",
       "      <td>0.107134</td>\n",
       "      <td>-0.168955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nessa</th>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>-0.048592</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>-0.001710</td>\n",
       "      <td>-0.012779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.014694</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.005298</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>-0.017630</td>\n",
       "      <td>-0.013593</td>\n",
       "      <td>-0.010662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similar</th>\n",
       "      <td>-0.029353</td>\n",
       "      <td>0.104890</td>\n",
       "      <td>0.046148</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>-0.020664</td>\n",
       "      <td>-0.362088</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.385523</td>\n",
       "      <td>-0.101135</td>\n",
       "      <td>-0.129910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132651</td>\n",
       "      <td>0.121552</td>\n",
       "      <td>-0.011716</td>\n",
       "      <td>0.094555</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.091967</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>-0.174501</td>\n",
       "      <td>0.106097</td>\n",
       "      <td>-0.151711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5   \\\n",
       "nice          -0.192781  0.428249  0.163581  0.179935  0.225132 -0.952760   \n",
       "sheeran       -0.007434  0.017735  0.014807  0.006584 -0.006980 -0.051160   \n",
       "discriminated  0.018713 -0.001984 -0.018869 -0.036782 -0.010157 -0.091945   \n",
       "typeonegative  0.003733 -0.011045 -0.014000 -0.012526 -0.000878 -0.005549   \n",
       "objection     -0.001627 -0.002641 -0.009385 -0.012354 -0.015650 -0.016477   \n",
       "professora    -0.022106  0.023572  0.024930  0.030998 -0.004385 -0.076845   \n",
       "slasher       -0.006892  0.013456  0.016699  0.001025 -0.010877 -0.035924   \n",
       "higher        -0.045284  0.134403  0.067103  0.064914  0.000813 -0.461645   \n",
       "nessa         -0.001683  0.006513  0.008208  0.000590 -0.001875 -0.048592   \n",
       "similar       -0.029353  0.104890  0.046148  0.029808 -0.020664 -0.362088   \n",
       "\n",
       "                     6         7         8         9   ...        90  \\\n",
       "nice           0.405003  1.078805 -0.556530 -0.293762  ...  0.495027   \n",
       "sheeran        0.011977  0.061541 -0.026173 -0.011591  ...  0.026947   \n",
       "discriminated  0.013388  0.086454 -0.028530 -0.025232  ...  0.051572   \n",
       "typeonegative  0.009506  0.036990 -0.008973  0.008513  ...  0.022193   \n",
       "objection     -0.000673  0.026697  0.014142  0.001016  ... -0.002933   \n",
       "professora     0.023927  0.100673 -0.032465 -0.018333  ...  0.010405   \n",
       "slasher        0.001811  0.038645 -0.014912 -0.003330  ...  0.008048   \n",
       "higher         0.078339  0.509531 -0.162674 -0.164502  ...  0.196889   \n",
       "nessa          0.015787  0.042553 -0.001710 -0.012779  ...  0.015412   \n",
       "similar        0.030800  0.385523 -0.101135 -0.129910  ...  0.132651   \n",
       "\n",
       "                     91        92        93        94        95        96  \\\n",
       "nice           0.256294  0.081433  0.399876  0.870979  0.357137  0.007828   \n",
       "sheeran        0.002599  0.000509  0.011770  0.038903  0.019126  0.006056   \n",
       "discriminated  0.033785 -0.001509  0.012612  0.063108  0.020124  0.013529   \n",
       "typeonegative  0.005509 -0.007446  0.009933  0.006811  0.012697 -0.017803   \n",
       "objection      0.014575 -0.001871 -0.003120  0.000940  0.004874 -0.007170   \n",
       "professora     0.015398 -0.006973  0.032991  0.060261  0.034890  0.007864   \n",
       "slasher        0.001483 -0.004797  0.004991  0.030776  0.014187  0.008514   \n",
       "higher         0.143719  0.009727  0.136243  0.330119  0.117404  0.006872   \n",
       "nessa          0.014694  0.012261  0.005298  0.023851  0.006722  0.014951   \n",
       "similar        0.121552 -0.011716  0.094555  0.242424  0.091967  0.019743   \n",
       "\n",
       "                     97        98        99  \n",
       "nice          -0.456908  0.254847 -0.234902  \n",
       "sheeran       -0.015366 -0.003382 -0.010420  \n",
       "discriminated -0.036124  0.023897 -0.022599  \n",
       "typeonegative -0.009518  0.023444 -0.012139  \n",
       "objection     -0.018893  0.003600 -0.008676  \n",
       "professora    -0.051127  0.015242 -0.035894  \n",
       "slasher       -0.015796  0.005368 -0.021430  \n",
       "higher        -0.227228  0.107134 -0.168955  \n",
       "nessa         -0.017630 -0.013593 -0.010662  \n",
       "similar       -0.174501  0.106097 -0.151711  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting the word vectors from the model\n",
    "word_vectors = (pd.DataFrame([model.wv.get_vector(str(n)) for n in model.wv.key_to_index], index = model.wv.key_to_index))\n",
    "\n",
    "print(df_w2v.shape)\n",
    "word_vectors.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
